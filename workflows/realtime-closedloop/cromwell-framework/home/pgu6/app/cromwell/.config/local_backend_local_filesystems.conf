include required(classpath("application"))

# Cromwell HTTP server settings
webservice {
  port = 9005
  ##interface = localhost
  interface = 0.0.0.0
}

# Cromwell "system" settings
system {
  #abort-jobs-on-terminate = false
  
  # If 'true', a SIGTERM or SIGINT will trigger Cromwell to attempt to gracefully shutdown in server mode,
  # in particular clearing up all queued database writes before letting the JVM shut down.
  # The shutdown is a multi-phase process, each phase having its own configurable timeout. See the Dev Wiki for more details.
  #graceful-server-shutdown = true
  #
  # Cromwell will cap the number of running workflows at N
  max-concurrent-workflows = 50
  #max-concurrent-workflows = 5000
   
  # Cromwell will launch up to N submitted workflows at a time, regardless of how many open workflow slots exist
  max-workflow-launch-count = 10
  ##max-workflow-launch-count = 50
  
  # Number of seconds between workflow launches
  new-workflow-poll-rate = 2
  #new-workflow-poll-rate = 20
  
  # Since the WorkflowLogCopyRouter is initialized in code, this is the number of workers
  #number-of-workflow-log-copy-workers = 10
  
  # Default number of cache read workers
  number-of-cache-read-workers = 5
  #number-of-cache-read-workers = 2

}

backend {
  LocalBackend {

      # The actor that runs the backend. In this case, it's the Shared File System (SFS) ConfigBackend.
      actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"

      # The backend custom configuration.
      config {

        # Optional limits on the number of concurrent jobs
        #concurrent-job-limit = 5
        concurrent-job-limit = 40

        # If true submits scripts to the bash background using "&". Only usefull for dispatchers that do NOT submit
        # the job and then immediately return a scheduled job id.
        run-in-background = true

        # `temporary-directory` creates the temporary directory for commands.
        #
        # If this value is not set explicitly, the default value creates a unique temporary directory, equivalent to:
        # temporary-directory = "$(mktemp -d \"$PWD\"/tmp.XXXXXX)"
        #
        # The expression is run from the execution directory for the script. The expression must create the directory
        # if it does not exist, and then return the full path to the directory.
        #
        # To create and return a non-random temporary directory, use something like:
        # temporary-directory = "$(mkdir -p /tmp/mydir && echo /tmp/mydir)"

        # `script-epilogue` configures a shell command to run after the execution of every command block.
        #
        # If this value is not set explicitly, the default value is `sync`, equivalent to:
        # script-epilogue = "sync"
        #
        # To turn off the default `sync` behavior set this value to an empty string:
        # script-epilogue = ""

        # `glob-link-command` specifies command used to link glob outputs, by default using hard-links.
        # If filesystem doesn't allow hard-links (e.g., beeGFS), change to soft-links as follows:
        # glob-link-command = "ln -sL GLOB_PATTERN GLOB_DIRECTORY"

        # The list of possible runtime custom attributes.
        runtime-attributes = """
        String? docker
        String? docker_user
        """

        # Submit string when there is no "docker" runtime attribute.
        submit = "/usr/bin/env bash ${script}"

          ## -v /labs/mahmoudilab/synergy-rt-preproc/CanlabCore/:/home/pgu6/realtime-closedloop/CanlabCore/  \
          ## -v /labs/mahmoudilab/synergy-rt-preproc/spm12/:/home/pgu6/realtime-closedloop/spm12/  \
          ## -v /labs/mahmoudilab/synergy-rt-preproc/run_RT_Preproc.sh:/home/pgu6/realtime-closedloop/run_RT_Preproc.sh:ro  \
          ## -v /labs/mahmoudilab/synergy-rt-preproc/RT_Preproc:/home/pgu6/realtime-closedloop/RT_Preproc  \
        # Submit string when there is a "docker" runtime attribute.
        submit-docker = """
        docker run \
          --rm -i \
          ${"--user " + docker_user} \
          --entrypoint ${job_shell} \
          -v ${cwd}:${docker_cwd} \
          -v /labs/mahmoudilab/synergy-rt-preproc/CanlabCore/:/home/pgu6/realtime-closedloop/CanlabCore/  \
          -v /labs/mahmoudilab/synergy-rt-preproc/spm12/:/home/pgu6/realtime-closedloop/spm12/  \
          -v /labs/mahmoudilab/synergy-rt-preproc/run_RT_Preproc.sh:/home/pgu6/realtime-closedloop/run_RT_Preproc.sh:ro  \
          -v /labs/mahmoudilab/synergy-rt-preproc/RT_Preproc:/home/pgu6/realtime-closedloop/RT_Preproc  \
          ${docker} ${script}
        """

        # Root directory where Cromwell writes job results.  This directory must be
        # visible and writeable by the Cromwell process as well as the jobs that Cromwell
        # launches.
        ##root = "/labs/mahmoudilab/synergy-wf-executions"
        root = "/labs/sharmalab/cloudypipelines/cromwell/cromwell-executions"
        ## root = "/tmp/cromwell-executions"


        # Root directory where Cromwell writes job results in the container. This value
        # can be used to specify where the execution folder is mounted in the container.
        # it is used for the construction of the docker_cwd string in the submit-docker
        # value above.
        dockerRoot = "/cromwell-executions"

        # File system configuration.
        filesystems {

          # For SFS backends, the "local" configuration specifies how files are handled.
          local {

            # Try to hard link (ln), then soft-link (ln -s), and if both fail, then copy the files.
            localization: [
              "copy", "soft-link", "hard-link"
            ]

            # Call caching strategies
            caching {
              # When copying a cached result, what type of file duplication should occur.
              # For more information check: https://cromwell.readthedocs.io/en/stable/backends/HPC/#shared-filesystem
              duplication-strategy: [
                "copy", "hard-link", "soft-link"
              ]

              # Strategy to determine if a file has been used before.
              # For extended explanation and alternative strategies check: https://cromwell.readthedocs.io/en/stable/Configuring/#call-caching
              hashing-strategy: "md5"

              # When true, will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash.
              # If false or the md5 does not exist, will proceed with the above-defined hashing strategy.
              check-sibling-md5: false
            }
          } 

        }

        # The defaults for runtime attributes if not provided.
        default-runtime-attributes {
          failOnStderr: false
          continueOnReturnCode: 0
        }
      }
    }
  
  
}
####
docker {
  hash-lookup {
    # Set this to match your available quota against the Google Container Engine API
    #gcr-api-queries-per-100-seconds = 1000
    
    # Time in minutes before an entry expires from the docker hashes cache and needs to be fetched again
    #cache-entry-ttl = "20 minutes"
    cache-entry-ttl = "30 minutes"
    
    # Maximum number of elements to be kept in the cache. If the limit is reached, old elements will be removed from the cache
    cache-size = 100
    #cache-size = 200
    # How should docker hashes be looked up. Possible values are "local" and "remote"
    # "local": Lookup hashes on the local docker daemon using the cli
    # "remote": Lookup hashes on docker hub and gcr
    method = "local"
    ##method = "remote"
  } 
}

#####
database {
  profile = "slick.jdbc.PostgresProfile$"

  db {
    driver = "org.postgresql.Driver"
    url = "jdbc:postgresql://localhost:5488/cromwell"
    ##url = "jdbc:postgresql://cromwell-7.priv.bmi.emory.edu:5488/cromwell"
    user = "cromwell"
    password = "cromwell"
    port = 5488
    connectionTimeout = 5000
  }

  # For batch inserts the number of inserts to send to the DB at a time
  # insert-batch-size = 2000

  migration {
    # For databases with a very large number of symbols, selecting all the rows at once can generate a variety of
    # problems. In order to avoid any issue, the selection is paginated. This value sets how many rows should be
    # retrieved and processed at a time, before asking for the next chunk.
    #read-batch-size = 100000

    # Because a symbol row can contain any arbitrary wdl value, the amount of metadata rows to insert from a single
    # symbol row can vary from 1 to several thousands (or more). To keep the size of the insert batch from growing out
    # of control we monitor its size and execute/commit when it reaches or exceeds writeBatchSize.
    #write-batch-size = 100000
  }

  # To customize the metadata database connection, create a block under `database` with the metadata database settings.
  #
  # For example, the default database stores all data in memory. This commented out block would store `metadata` in an
  # hsqldb file, without modifying the internal engine database connection.
  #
  # The value `${uniqueSchema}` is always replaced with a unqiue UUID on each cromwell startup.
  #
  # This feature should be considered experimental and likely to change in the future.

  #metadata {
  #  profile = "slick.jdbc.HsqldbProfile$"
  #  db {
  #    driver = "org.hsqldb.jdbcDriver"
  #    url = "jdbc:hsqldb:file:metadata-${uniqueSchema};shutdown=false;hsqldb.tx=mvcc"
  #    connectionTimeout = 3000
  #  }
  #}
}
